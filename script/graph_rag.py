# Neo4j Graph + Llama 3.1ì„ ì´ìš©í•œ í–¥ìˆ˜ ì¶”ì²œ RAG ì‹œìŠ¤í…œ

import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:512"

import transformers
import torch
import gc
import json
from typing import List, Dict, Tuple, Optional
from retrieval import Neo4jRetrieval
import re
from dotenv import load_dotenv
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from peft import PeftModel

# Neo4j ì—°ê²° ì •ë³´
URI = "bolt://localhost:7687"
USERNAME = "neo4j"
PASSWORD = "password"

load_dotenv()

class GraphRAG:
    """Graph RAG ì‹œìŠ¤í…œ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, neo4j_uri: str = None, neo4j_user: str = None, neo4j_password: str = None, model_id: str = None):
        """GraphRAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        import os
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°’ ì½ê¸° (íŒŒë¼ë¯¸í„°ê°€ ì—†ìœ¼ë©´)
        neo4j_uri = neo4j_uri or os.getenv('NEO4J_URI')
        neo4j_user = neo4j_user or os.getenv('NEO4J_USER')
        neo4j_password = neo4j_password or os.getenv('NEO4J_PASSWORD')
        model_id = model_id or os.getenv('MODEL_ID')
        hf_token = os.getenv('HF_TOKEN')
        self.retrieval = Neo4jRetrieval(neo4j_uri, neo4j_user, neo4j_password)
        self.model_id = model_id
        
        base_model_path = model_id  # .envì—ì„œ ì½ì€ base_model ê²½ë¡œ
        adapter_path = "/home/shcho95/yjllm/llama3_8b/weight/perfume_llama3_8B_v0"  # ì–´ëŒ‘í„° ê²½ë¡œ

        model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            token=hf_token
        )
        model = PeftModel.from_pretrained(model, adapter_path)
        tokenizer = AutoTokenizer.from_pretrained(base_model_path, token=hf_token)

        self.pipeline = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            device_map="auto",
            torch_dtype=torch.float16
        )
        print("âœ… GraphRAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def _extract_generated_response(self, full_output: str, prompt: str) -> str:
        """ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ ì‹¤ì œ ë‹µë³€ë§Œ ì¶”ì¶œ"""
        # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ì„ ì œê±°
        if prompt in full_output:
            response = full_output.replace(prompt, "").strip()
        else:
            response = full_output.strip()
        
        # ë¶ˆí•„ìš”í•œ íƒœê·¸ë‚˜ ë°˜ë³µëœ ë‚´ìš© ì œê±°
        response = re.sub(r'<[^>]+>', '', response)  # HTML íƒœê·¸ ì œê±°
        response = re.sub(r'\n+', '\n', response)  # ì—°ì†ëœ ì¤„ë°”ê¿ˆ ì •ë¦¬
        
        return response.strip()
    
    def ask(self, user_query: str) -> str:
        """ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ í–¥ìˆ˜ ì¶”ì²œ ì‘ë‹µ ìƒì„±"""
        try:
            # Graph RAG ë°©ì‹ìœ¼ë¡œ ê²€ìƒ‰
            graph_rag_response = self.retrieval.search_graph_rag(user_query)
            
            # 3. ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ì‘ì„±
            prompt = self._create_prompt(user_query, graph_rag_response)
            
            # 4. LLM ì‘ë‹µ ìƒì„±
            outputs = self.pipeline(
                prompt,
                max_new_tokens=300,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=self.pipeline.tokenizer.eos_token_id
            )
            
            # 5. ì‘ë‹µ í›„ì²˜ë¦¬
            full_response = outputs[0]["generated_text"]
            clean_response = self._extract_generated_response(full_response, prompt)
            
            print("===== LLMì— ì „ë‹¬ë˜ëŠ” ìµœì¢… ì»¨í…ìŠ¤íŠ¸ =====")
            print(graph_rag_response)
            
            return clean_response
            
        except Exception as e:
            print(f"âŒ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. í–¥ìˆ˜ ì¶”ì²œ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
    
    def _create_prompt(self, user_query: str, context: str) -> str:
        """í–¥ìˆ˜ ì¶”ì²œì„ ìœ„í•œ ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        # ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
        if not context or context == "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.":
            return f"""ë‹¹ì‹ ì€ ì¹œê·¼í•œ í–¥ìˆ˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì‚¬ìš©ì ì§ˆë¬¸: {user_query}

ì£„ì†¡í•˜ì§€ë§Œ ìš”ì²­í•˜ì‹  ì¡°ê±´ì— ì •í™•íˆ ë§ëŠ” í–¥ìˆ˜ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. 
ë‹¤ë¥¸ í‚¤ì›Œë“œë‚˜ ë” êµ¬ì²´ì ì¸ ì¡°ê±´ì„ ë§ì”€í•´ ì£¼ì‹œë©´ ë” ì¢‹ì€ ì¶”ì²œì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì¶”ì²œ:"""
        
        # ì¼ë°˜ì ì¸ ê²½ìš° í”„ë¡¬í”„íŠ¸
        return f"""ë‹¹ì‹ ì€ ì „ë¬¸ì ì´ê³  ì¹œê·¼í•œ í–¥ìˆ˜ ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

        ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ Graph RAG ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í–¥ìˆ˜ ì¶”ì²œì„ í•œêµ­ì–´ë¡œ í•´ì£¼ì„¸ìš”:
        ë‹µë³€ì— í¬í•¨í•  ë‚´ìš© :
        - í–¥ìˆ˜ì˜ êµ¬ì²´ì ì¸ íŠ¹ì§•(ë¸Œëœë“œ, í–¥ì¡°, í‰ì  ë“±) í¬í•¨
        - ì£¼ì–´ì§„ Graph RAG ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì ê·¹ í™œìš©í•  ê²ƒ
        - ê±°ì§“ ì •ë³´ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ì•ŠìŒ
        - ê²€ìƒ‰ ê²°ê³¼ì˜ ì ìˆ˜ì™€ ê²½ë¡œ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì„¤ëª…í•  ê²ƒ

        ì‚¬ìš©ì ì§ˆë¬¸: {user_query}

        Graph RAG ê²€ìƒ‰ ê²°ê³¼:
        {context}


        í–¥ìˆ˜ ì¶”ì²œ:
        """
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        print("ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        if hasattr(self, 'pipeline') and self.pipeline:
            del self.pipeline
            gc.collect()
            torch.cuda.empty_cache()
        
        if hasattr(self, 'retrieval') and self.retrieval:
            self.retrieval.close()
        
        print("âœ… ì •ë¦¬ ì™„ë£Œ!")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸŒ¸ Graph RAG í–¥ìˆ˜ ì¶”ì²œ ì‹œìŠ¤í…œ ğŸŒ¸")
    print("=" * 50)
    
    # GraphRAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag_system = GraphRAG()
    
    try:
        # Neo4j ì—°ê²° í…ŒìŠ¤íŠ¸
        if not rag_system.retrieval.test_connection():
            print("âŒ Neo4j ì—°ê²° ì‹¤íŒ¨. ì„œë²„ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return
        print("âœ… Neo4j ì—°ê²° ì„±ê³µ")
        
        # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
        test_questions = [
            "woodyí•œ í–¥ìˆ˜ ì°¾ê³  ìˆì–´", 
            "Chanel í–¥ìˆ˜ ì¤‘ì— ë­ê°€ ì¢‹ì„ê¹Œ?",
        ]
        
        print("\n=== í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤ ===")
        for i, question in enumerate(test_questions, 1):
            print(f"\n[í…ŒìŠ¤íŠ¸ {i}] ì§ˆë¬¸: {question}")
            response = rag_system.ask(question)
            print(f"ğŸ¤– ì¶”ì²œ ê²°ê³¼:\n{response}")
            print("-" * 50)
        
        # ëŒ€í™”í˜• ëª¨ë“œ
        print("\n=== ëŒ€í™”í˜• ëª¨ë“œ ì‹œì‘ ===")
        print("í–¥ìˆ˜ì— ê´€í•œ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”! (ì¢…ë£Œ: 'quit')")
        
        while True:
            try:
                user_input = input("\nğŸ’­ ì§ˆë¬¸: ").strip()
                if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'q']:
                    break
                
                if user_input:
                    response = rag_system.ask(user_input)
                    print(f"\nğŸ¤– ì¶”ì²œ ê²°ê³¼:\n{response}")
                    
            except KeyboardInterrupt:
                print("\nì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    finally:
        rag_system.cleanup()

if __name__ == "__main__":
    main()