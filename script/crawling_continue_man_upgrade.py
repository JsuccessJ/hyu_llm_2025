import json
import time
import random
import os
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium_stealth import stealth

def scrape_perfume(url, index=None):
    options = Options()
    options.add_argument("--no-sandbox")
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--remote-debugging-port=0")  # í¬íŠ¸ ì¶©ëŒ ë°©ì§€
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                         "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36")

    driver = webdriver.Chrome(options=options)

    stealth(driver,
            languages=["en-US", "en"],
            vendor="Google Inc.",
            platform="Win32",
            webgl_vendor="Intel Inc.",
            renderer="Intel Iris OpenGL Engine",
            fix_hairline=True,
    )

    wait = WebDriverWait(driver, 7)

    try:
        driver.get(url)

        if "ì ì‹œë§Œ ê¸°ë‹¤ë¦¬ì‹­ì‹œì˜¤" in driver.page_source or "Just a moment..." in driver.page_source:
            raise Exception("âš ï¸ Cloudflare ì°¨ë‹¨ í˜ì´ì§€ ê°ì§€ë¨.")

        wait.until(EC.presence_of_element_located((By.ID, "main-content")))
        data = {'url': url}

        name_elem = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'h1[itemprop="name"]')))
        data['name'] = name_elem.text.strip().split('\n')[0]
        try:
            small_tag = name_elem.find_element(By.TAG_NAME, 'small')
            data['target'] = small_tag.text.strip()
        except:
            data['target'] = None

        try:
            brand_elem = driver.find_element(By.CSS_SELECTOR, 'p[itemprop="brand"] a[itemprop="url"]')
            data['brand_name'] = brand_elem.find_element(By.CSS_SELECTOR, 'span[itemprop="name"]').text.strip()
        except:
            data['brand_name'] = None

        data['accords'] = []
        for bar in driver.find_elements(By.CSS_SELECTOR, 'div.accord-bar'):
            accord = bar.text.strip()
            style = bar.get_attribute('style') or ""
            strength = style.split('width:')[-1].split(';')[0].strip() if "width:" in style else None
            data['accords'].append({'accord': accord, 'strength': strength})

        for field, selector in [
            ('rating_value', 'span[itemprop="ratingValue"]'),
            ('best_rating', 'span[itemprop="bestRating"]'),
            ('rating_count', 'span[itemprop="ratingCount"]')
        ]:
            try:
                data[field] = driver.find_element(By.CSS_SELECTOR, selector).text.strip()
            except:
                data[field] = None

        try:
            data['review_count'] = driver.find_element(By.CSS_SELECTOR, 'meta[itemprop="reviewCount"]').get_attribute('content').strip()
        except:
            data['review_count'] = None

        try:
            data['short_description'] = driver.find_element(By.CSS_SELECTOR, 'div[itemprop="description"] > p:first-of-type').text.strip()
        except:
            data['short_description'] = None

        detailed = [p.text.strip() for p in driver.find_elements(By.CSS_SELECTOR, 'div.fragrantica-blockquote > p') if p.text.strip()]
        data['detailed_description'] = "\n".join(detailed) if detailed else None

        for cat, labels in [('seasonal', ['winter', 'spring', 'summer', 'fall']), ('time_of_day', ['day', 'night'])]:
            data[cat] = {}
            for lbl in labels:
                try:
                    legend = driver.find_element(By.XPATH, f"//span[@class='vote-button-legend' and normalize-space(text())='{lbl}']")
                    colored = legend.find_element(By.XPATH, "../following-sibling::div[contains(@class,'voting-small-chart-size')]/div/div")
                    style = colored.get_attribute('style') or ""
                    pct = style.split('width:')[-1].split(';')[0].strip() if "width:" in style else None
                except:
                    pct = None
                data[cat][lbl] = pct

        for field in ['Pros', 'Cons']:
            try:
                header = driver.find_element(By.XPATH, f"//h4[normalize-space(text())='{field}']")
                section = header.find_element(By.XPATH, "./ancestor::div[contains(@class,'small-12') and contains(@class,'medium-6')]")
                spans = section.find_elements(By.XPATH, ".//span")
                items = [s.text.strip() for s in spans if s.text.strip() and not s.text.strip().replace(",", "").isdigit()]
                data[field.lower()] = items
            except:
                data[field.lower()] = []

        return data

    finally:
        if index is not None and "ì ì‹œë§Œ ê¸°ë‹¤ë¦¬ì‹­ì‹œì˜¤" in driver.page_source:
            with open(f"blocked_html_{index}.html", "w", encoding="utf-8") as f:
                f.write(driver.page_source)
        driver.quit()

def load_perfume_links(path):
    try:
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f).get('perfume_links', [])
    except:
        return []

def get_processed_urls(result_file):
    """ê¸°ì¡´ ê²°ê³¼ íŒŒì¼ì—ì„œ ì´ë¯¸ ì²˜ë¦¬ëœ(ì„±ê³µí•œ) URLë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    processed_urls = set()
    
    if os.path.exists(result_file):
        try:
            with open(result_file, 'r', encoding='utf-8') as f:
                result = json.load(f)
                
            # ì„±ê³µí•œ URLë“¤ ì¶”ê°€
            for perfume in result.get('perfumes_data', []):
                if 'url' in perfume:
                    processed_urls.add(perfume['url'])
                    
            print(f"âœ… ì´ë¯¸ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ URL: {len(processed_urls)}ê°œ")
            
        except Exception as e:
            print(f"âš ï¸ ê²°ê³¼ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
    else:
        print("ğŸ“‚ ê¸°ì¡´ ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
    
    return processed_urls

def get_remaining_urls(all_urls, processed_urls):
    """ì „ì²´ URLì—ì„œ ì•„ì§ ì²˜ë¦¬ë˜ì§€ ì•Šì€ URLë“¤ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤."""
    remaining_urls = []
    
    for i, url in enumerate(all_urls):
        if url not in processed_urls:
            remaining_urls.append((i + 1, url))  # ì›ë˜ ì¸ë±ìŠ¤ì™€ í•¨ê»˜ ì €ì¥
    
    return remaining_urls

def save_single_result(data, filename):
    if os.path.exists(filename):
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                result = json.load(f)
        except:
            result = {'successful_count': 0, 'failed_count': 0, 'total_count': 0, 'perfumes_data': [], 'failed_urls': []}
    else:
        result = {'successful_count': 0, 'failed_count': 0, 'total_count': 0, 'perfumes_data': [], 'failed_urls': []}

    if 'error' in data:
        result['failed_urls'].append(data)
        result['failed_count'] += 1
    else:
        result['perfumes_data'].append(data)
        result['successful_count'] += 1
    result['total_count'] = result['successful_count'] + result['failed_count']

    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    # ì„¤ì •
    links_file = "perfume_links_man.json"  # ì›ë³¸ ë§í¬ íŒŒì¼
    result_file = "perfumes_complete_man_data.json"  # ê²°ê³¼ ì €ì¥ íŒŒì¼
    MAX_RETRIES = 2  # ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ íšŸìˆ˜
    
    print("ğŸ”„ í¬ë¡¤ë§ ì¬ì‹œì‘ ì¤€ë¹„ ì¤‘...")
    
    # 1. ì „ì²´ URL ë¡œë“œ
    all_urls = load_perfume_links(links_file)
    print(f"ğŸ“‹ ì „ì²´ URL ê°œìˆ˜: {len(all_urls)}ê°œ")
    
    # 2. ì´ë¯¸ ì²˜ë¦¬ëœ URL í™•ì¸
    processed_urls = get_processed_urls(result_file)
    
    # 3. ë‚¨ì€ URLë“¤ë§Œ í•„í„°ë§
    remaining_urls = get_remaining_urls(all_urls, processed_urls)
    
    if not remaining_urls:
        print("ğŸ‰ ëª¨ë“  URLì´ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
        exit()
    
    print(f"â­ï¸ ë‚¨ì€ URL ê°œìˆ˜: {len(remaining_urls)}ê°œ")
    print(f"ğŸš€ {remaining_urls[0][0]}ë²ˆì§¸ URLë¶€í„° ì¬ì‹œì‘í•©ë‹ˆë‹¤.")
    
    # 4. ë‚¨ì€ URLë“¤ í¬ë¡¤ë§
    for original_index, url in remaining_urls:
        for i in range(MAX_RETRIES + 1):
            print(f"[{original_index}/{len(all_urls)}] í¬ë¡¤ë§ ì¤‘: {url} (ì‹œë„ {i+1}/{MAX_RETRIES+1})")
            try:
                result = scrape_perfume(url, index=original_index)
                save_single_result(result, result_file)
                print(f"âœ… ì„±ê³µ: {result.get('name')} [{result.get('brand_name')}]")
                break # ì„±ê³µ ì‹œ ì¬ì‹œë„ ë£¨í”„ íƒˆì¶œ
            except Exception as e:
                print(f"âŒ ì‹¤íŒ¨: {e}")
                if i < MAX_RETRIES:
                    wait_time = 300  # 5ë¶„
                    print(f"â³ {wait_time}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤... (ì¬ì‹œë„ {i+1}/{MAX_RETRIES})")
                    time.sleep(wait_time)
                else:
                    print(f"ğŸš« ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜({MAX_RETRIES+1})ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì´ URLì€ ì‹¤íŒ¨ë¡œ ê¸°ë¡í•©ë‹ˆë‹¤.")
                    error = {'url': url, 'error': f"ìµœëŒ€ ì¬ì‹œë„ í›„ì—ë„ ì‹¤íŒ¨: {str(e)}"}
                    save_single_result(error, result_file)

        # ë§ˆì§€ë§‰ URLì´ ì•„ë‹ˆë©´ ëŒ€ê¸°
        if (original_index, url) != remaining_urls[-1]:
            sleep_time = random.uniform(5, 8)
            print(f"ğŸ’¤ {sleep_time:.1f}ì´ˆ ëŒ€ê¸° ì¤‘...")
            time.sleep(sleep_time)

    print("\nğŸ‰ ì´ì–´ì„œ í¬ë¡¤ë§ ì™„ë£Œ!")
    
    # ìµœì¢… í†µê³„ ì¶œë ¥
    if os.path.exists(result_file):
        try:
            with open(result_file, 'r', encoding='utf-8') as f:
                final_result = json.load(f)
            print(f"ğŸ“Š ìµœì¢… í†µê³„:")
            print(f"   âœ… ì„±ê³µ: {final_result.get('successful_count', 0)}ê°œ")
            print(f"   âŒ ì‹¤íŒ¨: {final_result.get('failed_count', 0)}ê°œ")
            print(f"   ğŸ“ˆ ì „ì²´: {final_result.get('total_count', 0)}ê°œ")
        except:
            pass 