import os
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_community.llms import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.embeddings import HuggingFaceEmbeddings
from dotenv import load_dotenv
from huggingface_hub import login
from prompt_loader import *

load_dotenv()
hf_token = os.getenv("HUGGINGFACEHUB_API_TOKEN")
login(token=hf_token)

MODEL_ID = "meta-llama/Llama-3.2-3B-Instruct"
print("ëª¨ë¸ ë¡œë”© ì¤‘...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_auth_token=hf_token)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    device_map="auto",
    torch_dtype="auto",
    use_auth_token=hf_token,
)
print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

llm_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=512,
    temperature=0.7,
    do_sample=True,
)
llm = HuggingFacePipeline(pipeline=llm_pipeline)

EMBED_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
print("ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
embed_model = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)
print("ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

print("FAISS ë²¡í„° DB ë¡œë”© ì¤‘...")
vector_store = FAISS.load_local(
    "./perfume_faiss_index",
    embed_model,
    allow_dangerous_deserialization=True
)
print("ë²¡í„° DB ë¡œë”© ì™„ë£Œ")

retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 3})

# ì¤€ë¹„
####################################################################### í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
prompts = load_prompts_from_yaml("./prompts.yaml")

def create_qa_chain_with_prompt(prompt: PromptTemplate):
    return RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt}
    )
def ask(qa_chain, query: str):
    result = qa_chain({"query": query})
    print("ğŸ’¬ ì§ˆë¬¸:", query)
    print("ğŸ§  ë‹µë³€:", result['result'])
    print("\nğŸ“š ì°¸ì¡° ë¬¸ì„œ:")
    if result["source_documents"]:
        for doc in result["source_documents"]:
            brand = doc.metadata.get("brand_name", "ì•Œ ìˆ˜ ì—†ìŒ")
            rating = doc.metadata.get("rating_value", "?")
            print(f"- ë¸Œëœë“œ: {brand} | í‰ì : {rating}")
    else:
        print("- ì°¸ì¡° ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    prompts = load_prompts_from_yaml("./prompts.yaml")

    while True:
        query = input("ì§ˆë¬¸ ì…ë ¥ (ì¢…ë£ŒëŠ” exit): ").strip()
        if query.lower() in ("exit", "quit"):
            print("í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
            break
        if not query:
            continue

        prompt_type = classify_query(query)
        kwargs = {}

        if prompt_type in ("perfume_summary", "note_description"):
            kwargs["perfume_name"] = "AlthaÃ¯r"
        elif prompt_type == "recommendation_scenario":
            kwargs["scenario_description"] = query
        elif prompt_type == "brand_specific":
            kwargs["brand_name"] = "Parfums de Marly"
            kwargs["ingredient"] = "vanilla"
        elif prompt_type == "season_time":
            kwargs["season_or_time"] = "spring"
        elif prompt_type == "mood_atmosphere":
            kwargs["mood_description"] = "sweet and warm"

        prompt_template = prompts.get(prompt_type)
        if prompt_template is None:
            prompt_template = prompts.get("basic_prompt")
            if prompt_template is None:
                print("ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break

        # ë³€ìˆ˜ ì¼ë¶€ë§Œ ë¯¸ë¦¬ ì±„ìš´ PromptTemplate ê°ì²´ ìƒì„±
        prompt_with_vars = prompt_template.partial(**kwargs)

        qa_chain = create_qa_chain_with_prompt(prompt_with_vars)
        ask(qa_chain, query)