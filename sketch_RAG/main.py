import os
import yaml
from dotenv import load_dotenv
from huggingface_hub import login
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_community.llms import HuggingFacePipeline
from prompt_loader import load_prompts_from_yaml
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.prompts import PromptTemplate
from langchain.chains.retrieval import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
import inspect
"""
pip install --upgrade transformers torch langchain langchain-core
LangChain ë²„ì „ í™•ì¸ # pip show langchain
Name: langchain
Version: 0.3.25
Summary: Building applications with LLMs through composability
Home-page: 
Author: 
Author-email: 
License: MIT
Location: /home/dibaeck/sketch/anaconda3/envs/dibk311/lib/python3.11/site-packages
Requires: langchain-core, langchain-text-splitters, langsmith, pydantic, PyYAML, requests, SQLAlchemy
Required-by: langchain-community
"""
########## utils
class InputRequiredError(Exception):
    pass

# ì„¤ì • ë¶ˆëŸ¬ì˜¤ê¸°
def load_config(path="config.yaml"):
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)

# ì‚¬ìš©ì ì…ë ¥ ë˜ëŠ” ê¸°ë³¸ê°’ ë°˜í™˜
def get_input_or_default(prompt_text, default_value=None):
    user_input = input(f"{prompt_text}{f' (ê¸°ë³¸ê°’: {default_value})' if default_value is not None else ''}: ").strip()
    if not user_input and default_value is None:
        raise InputRequiredError(f"'{prompt_text}' ì…ë ¥ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    return user_input if user_input else default_value

#############################
# ëª¨ë¸ ë° ë²¡í„° DB ë¡œë”©
def setup_models_and_vectorstore():
    load_dotenv()
    hf_token = os.getenv("HUGGINGFACEHUB_API_TOKEN")
    login(token=hf_token)

    MODEL_ID = "meta-llama/Llama-3.2-3B-Instruct"
    print("ëª¨ë¸ ë¡œë”© ì¤‘...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_auth_token=hf_token)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        device_map="auto",
        torch_dtype="auto",
        use_auth_token=hf_token,
    )
    print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

    llm_pipeline = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=512,
        temperature=0.7,
        do_sample=True,
    )
    llm = HuggingFacePipeline(pipeline=llm_pipeline)

    EMBED_MODEL_NAME = "BAAI/bge-m3"
    print("ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
    embed_model = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)
    print("ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

    print("FAISS ë²¡í„° DB ë¡œë”© ì¤‘...")
    vector_store = FAISS.load_local(
        "./perfume_faiss_index",
        embed_model,
        allow_dangerous_deserialization=True
    )
    print("ë²¡í„° DB ë¡œë”© ì™„ë£Œ")

    retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 3})
    # search_type="mmr", search_kwargs={"k": 5, "lambda_mult": 0.8} ì´ê±¸ë¡œ ì‹¤í—˜í•´ë³´ê¸° : mmrëŠ” LLM + RAGì—ì„œ context redundancyë¥¼ ì¤„ì—¬ ì„±ëŠ¥ í–¥ìƒì— ë„ì›€ëœë‹¤ê³  í•¨.
    # similarityëŠ” ë‹¨ìˆœ ìœ ì‚¬ë„ ê¸°ë°˜ì´ë¼ ë‹¤ì–‘ì„± ë¶€ì¡±í•  ìˆ˜ ìˆìŒ.

    return llm, retriever

# QA ì²´ì¸ ìƒì„±
"""
[ì§ˆë¬¸ â†’ ê²€ìƒ‰ â†’ ë¬¸ë§¥ ìƒì„± â†’ í”„ë¡¬í”„íŠ¸ êµ¬ì„± â†’ ë‹µë³€ ìƒì„±] Chainì„ ìƒì„±.
qa_chain.invoke({"input": query})ë¥¼ í•˜ë©´, retrieverê°€ ë²¡í„° ê²€ìƒ‰í•´ì„œ ê´€ë ¨ ë¬¸ì„œ ê°€ì ¸ì™€ì„œ promptí…œí”Œë¦¿ì— ë„£ìŒ.             # queryê°€ ì•„ë‹ˆë¼ inputìœ¼ë¡œ ë³€ìˆ˜ ì •ì˜í•´ì•¼í•¨....
LLMì— ë„£ì–´ ë‹µë³€ ìƒì„±í•˜ê³  ìƒì„±ëœ ë‹µë³€ê³¼ ì°¸ì¡° ë¬¸ì„œ ë¦¬í„´.

result, source_documents

"""
def create_qa_chain_with_prompt(prompt: PromptTemplate, llm, retriever):
    # ë¬¸ì„œë“¤ì„ í•˜ë‚˜ë¡œ í•©ì³ì„œ LLMì— ë„˜ê¸°ëŠ” stuff chain ìƒì„±
    stuff_chain = create_stuff_documents_chain(
        llm=llm,
        prompt=prompt,
        document_variable_name="context",
    )
    # retriever + stuff_chain ì„ í•©ì¹œ retrieval QA ì²´ì¸ ìƒì„±
    qa_chain = create_retrieval_chain(
        retriever=retriever,
        combine_docs_chain=stuff_chain,
    )
    return qa_chain

# ì§ˆë¬¸ ë° ë‹µë³€ í•¨ìˆ˜
def ask(qa_chain, query: str):
    result = qa_chain.invoke({"input": query})              # result.keys() :: ['input', 'context', 'answer']

    print("ğŸ’¬ ì§ˆë¬¸:", query)
    print("ğŸ§  ë‹µë³€:", result['answer'])
    
    context_docs = result["context"]

    if isinstance(context_docs, list):
        print("\nğŸ“š ì°¸ì¡° ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸:")
        for i, doc in enumerate(context_docs):
            print(f"\n--- ë¬¸ì„œ {i+1} ---")
            print("ë©”íƒ€ë°ì´í„°:", doc.metadata)
            print("ë³¸ë¬¸ ì¼ë¶€:", doc.page_content[:500])  # ì• 500ìë§Œ ì¶œë ¥
    else:
        # ê·¸ëƒ¥ ë¬¸ìì—´ì´ë©´ ì¼ë¶€ë§Œ ì¶œë ¥
        print("\nğŸ“š ì°¸ì¡° ë¬¸ì„œ ë‚´ìš©:")
        print(context_docs[:1000])

############################################# ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    prompts = load_prompts_from_yaml("./prompts.yaml")
    llm, retriever = setup_models_and_vectorstore()

    # basic_prompt í…œí”Œë¦¿ í•˜ë‚˜ë§Œ ì‚¬ìš©
    basic_prompt_template = prompts.get("basic_prompt")
    if set(basic_prompt_template.input_variables) != {"context", "input"}:
        raise ValueError("í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì€ 'context'ì™€ 'query'ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.")
    
    # ì²´ì¸ ìƒì„±
    qa_chain = create_qa_chain_with_prompt(
        prompt=basic_prompt_template,
        llm=llm,
        retriever=retriever
    )
    EXIT_COMMANDS = {"exit", "quit", "q", "ì¢…ë£Œ", "ê·¸ë§Œ"}
    try:
        while True:
            query = input('ì§ˆë¬¸ ì…ë ¥ (ì¢…ë£ŒëŠ” "ê·¸ë§Œ" ì…ë ¥ or ì…ë ¥í•˜ì§€ ì•Šê¸°): ').strip()
            if query.lower() in EXIT_COMMANDS:
                print("í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
                break
            if not query:
                print("ì…ë ¥ì´ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break

            ask(qa_chain, query)

    except KeyboardInterrupt:
        print("\nì‚¬ìš©ìì— ì˜í•´ í”„ë¡œê·¸ë¨ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        
